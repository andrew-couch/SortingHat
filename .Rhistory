unnest_tokens(bigram, `df`, token = "ngrams", n = 2) %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
filter(word1 == "said" | word2 == "said") %>% arrange(word1) %>% unique(word1, word2)
filter(word1 == "said" | word2 == "said") %>% arrange(word1) %>% unique(word1_
df %>%
unnest_tokens(bigram, `df`, token = "ngrams", n = 2) %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
filter(word1 == "said" | word2 == "said") %>% arrange(word1) %>% unique(word1)
df %>%
unnest_tokens(bigram, `df`, token = "ngrams", n = 2) %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
filter(word1 == "said" | word2 == "said") %>% arrange(word1)
df %>%
unnest_tokens(bigram, `df`, token = "ngrams", n = 2) %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
filter(word1 == "said" | word2 == "said") %>%
select(word1, word2) %>%
arrange() %>%
unique()
df %>%
unnest_tokens(bigram, `df`, token = "ngrams", n = 2) %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
filter(word1 == "said" | word2 == "said") %>%
select(word1, word2) %>%
arrange(word1) %>%
unique()
df %>%
unnest_tokens(bigram, `df`, token = "ngrams", n = 2) %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
filter(word1 == "said") %>%
select(word1, word2) %>%
unique() %>%
arrange(word1)
df %>%
unnest_tokens(bigram, `df`, token = "ngrams", n = 2) %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
filter(word1 == "said") %>%
select(word1, word2) %>%
unique() %>%
arrange(word2)
df %>%
unnest_tokens(bigram, `df`, token = "ngrams", n = 2) %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
filter(word1 == "said") %>%
select(word1, word2) %>%
unique() %>%
arrange(word2)
df %>%
unnest_tokens(bigram, `df`, token = "ngrams", n = 2) %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
filter(word1 == "said") %>%
select(word2) %>%
unique() %>%
arrange()
df %>%
unnest_tokens(bigram, `df`, token = "ngrams", n = 2) %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
filter(word1 == "said") %>%
select(word2) %>%
unique() %>%
arrange(word2)
df %>%
unnest_tokens(bigram, `df`, token = "ngrams", n = 2) %>%
separate(bigram, c("word1", "word2"), sep = " ")
df %>%
unnest_tokens(bigram, `df`, token = "ngrams", n = 2) %>%
separate(bigram, c("word1", "word2"), sep = " ") %>% select(word1, word2)
bigram <- df %>%
unnest_tokens(bigram, `df`, token = "ngrams", n = 2) %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
select(word1, word2)
bigram %>%
filter(word1 == "said")
bigram %>%
filter(word1 == "said") %>%
select(word2) %>%
unique() %>%
arrange()
bigram %>%
filter(word1 == "said") %>%
select(word2) %>%
unique() %>%
arrange(word2)
bigram %>%
filter(word1 == "said") %>%
select(word2) %>%
unique() %>%
rbind(bigram %>%
filter(word2 == "said") %>%
select(word1)) %>% arrange()
bigram %>%
filter(word1 == "said") %>%
select(word2) %>%
unique() %>%
rbind(bigram %>%
filter(word2 == "said") %>%
select(word1))
select(word1)
bigram %>%
filter(word1 == "said") %>%
select(word2) %>%
unique() %>%
rbind(bigram %>%
filter(word2 == "said") %>%
select(word1))
bigram %>%
filter(word1 == "said") %>%
select(word2) %>%
unique() %>%
rbind(bigram %>%
filter(word2 == "said") %>% select(word1))
bigram %>%
filter(word1 == "said") %>%
select(word2) %>%
unique() %>%
rbind(bigram %>%
filter(word2 == "said") %>% select(word1))
bigram %>%
filter(word1 == "said") %>%
select(word2) %>%
unique() %>%
rbind(bigram %>%
filter(word2 == "said") %>% select(word1) %>% unique())
rbind(bigram %>%
filter(word2 == "said") %>% select(word1) %>% unique())
rbind(bigram %>% filter(word2 == "said") %>% select(word1) %>% unique(),
bigram %>% filter(word1 == "said") %>% select(word2) %>% unique())
rbind(bigram %>% filter(word2 == "said") %>% select(word1) %>% unique(),
bigram %>% filter(word1 == "said") %>% select(word2) %>% unique())
rbind(bigram %>% filter(word2 == "said") %>% select(word1)
rbind(bigram %>% filter(word2 == "said") %>% select(word1),
bigram %>% filter(word1 == "said") %>% select(word2))
rbind(bigram %>% filter(word2 == "said") %>% select(word1),
bigram %>% filter(word1 == "said") %>% select(word2))
rbind(bigram %>% filter(word2 == "said") %>% select(word1),
bigram %>% filter(word1 == "said") %>% select(word2))
rbind(bigram %>% filter(word2 == "said") %>% select(word1),
bigram %>% filter(word1 == "said") %>% select(word2))
bigram %>% filter(word1 == "said" | word2 == "said")
bigram %>%
filter(word1 == "said" | word2 == "said") %>% rbind(word1, word2)
bigram %>%
filter(word1 == "said" | word2 == "said") %>% unique(word1, word2)
bigram %>%
filter(word1 == "said" | word2 == "said") %>% unique(word1, word2)
bigram %>%
filter(word1 == "said" | word2 == "said") %>% unique()
bigram %>%
filter(word1 == "said" | word2 == "said") %>% unique() %>% gather()
bigram %>%
filter(word1 == "said" | word2 == "said") %>%
unique() %>%
gather() %>%
select(value)
bigram %>%
filter(word1 == "said" | word2 == "said") %>%
unique() %>%
gather() %>%
select(value) %>%
unique()
bigram %>%
filter(word1 == "said" | word2 == "said") %>%
gather() %>%
select(value) %>%
unique()
bigram %>%
filter(word1 == "said" | word2 == "said")
bigram %>%
filter(word1 == "said" | word2 == "said") %>%
gather()
bigram %>%
filter(word1 == "said" | word2 == "said") %>%
gather() %>%
select(value) %>%
unique() %>%
arrange()
bigram %>%
filter(word1 == "said" | word2 == "said") %>%
gather() %>%
select(value) %>%
unique() %>%
arrange(value)
df %>%
unnest_tokens(bigram, `df`, token = "ngrams", n = 2) %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
select(word1, word2) %>%
filter(word1 == "said" | word2 == "said") %>%
gather() %>%
select(value) %>%
unique() %>%
arrange(value)
df %>% rbind(goblet_of_fire)
df <- df %>% rbind(goblet_of_fire)
rbind(df, goblet_of_fire)
df <- rbind(df, goblet_of_fire)
View(df)
df <- philosophers_stone
df <- as.data.frame(df)
goblet_of_fire <- goblet_of_fire
goblet_of_fire <- as.data.frame(goblet_of_fire)
View(goblet_of_fire)
df <- df %>% rbind(goblet_of_fire)
df <-  rbind(df,goblet_of_fire)
View(goblet_of_fire)
View(df)
df <- philosophers_stone
df <- philosophers_stone %>% as.data.frame()
df <- philosophers_stone %>% as.data.frame()
df %>%
unnest_tokens(bigram, `df`, token = "ngrams", n = 2) %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
select(word1, word2) %>%
filter(word1 == "said" | word2 == "said") %>%
gather() %>%
select(value) %>%
unique() %>%
arrange(value)
View(df)
View(df)
df %>%
unnest_tokens(bigram, token = "ngrams", n = 2) %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
select(word1, word2) %>%
filter(word1 == "said" | word2 == "said") %>%
gather() %>%
select(value) %>%
unique() %>%
arrange(value)
colnames(df) <- "text"
df %>%
unnest_tokens(bigram,`text`, token = "ngrams", n = 2) %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
select(word1, word2) %>%
filter(word1 == "said" | word2 == "said") %>%
gather() %>%
select(value) %>%
unique() %>%
arrange(value)
df <- goblet_of_fire %>% as.data.frame()
colnames(df) <- "text"
df %>%
unnest_tokens(bigram,`text`, token = "ngrams", n = 2) %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
select(word1, word2) %>%
filter(word1 == "said" | word2 == "said") %>%
gather() %>%
select(value) %>%
unique() %>%
arrange(value)
library(rattle)
rattle()
library(rattle)
rattle()
library(tidyverse)
library(tidyverse)
df <- data.frame(house = c("g", "s", "h", "r"), values = (.5,.1,.1,.3))
df <- data.frame(house = c("g", "s", "h", "r"), values = (.5,.1,.1,.3))
df <- data.frame("house" = c("g", "s", "h", "r"), "values" = (.5,.1,.1,.3))
df <- data.frame("house" = c("g", "s", "h", "r"),
"values" = c(.5,.1,.1,.3))
ggplot(data = df, mapping = aes(x = house, y = values)) + geom_bar()
ggplot(data = df, mapping = aes(x = house, y = values)) + geom_col()
ggplot(data = df, mapping = aes(x = house, y = values)) + geom_col() + coord_polar()
ggplot(data = df, mapping = aes(x = house, y = values, color = house)) + geom_col() + coord_polar()
ggplot(data = df, mapping = aes(x = house, y = values, color = house, fill = house)) + geom_col() + coord_polar()
ggplot(data = df, mapping = aes(x = house, y = values, color = house, fill = house)) + geom_line() + coord_polar()
ggplot(data = df, mapping = aes(x = house, y = values, color = house, fill = house, group = house)) + geom_line() + coord_polar()
ggplot(data = df, mapping = aes(x = house, y = values, color = house, fill = house)) + geom_line() + coord_polar()
ggplot(data = df, mapping = aes(x = house, y = values, color = house, fill = house)) + geom_col() + coord_polar()
ggplot(data = df, mapping = aes(x = house, y = values, color = house, fill = house)) + geom_col() + coord_polar(theta = "y")
ggplot(data = df, mapping = aes(x = house, y = values, color = house, fill = house)) + geom_col() + coord_polar(theta = "x")
ggplot(data = df, mapping = aes(x = house, y = values, color = house, fill = house)) + geom_col(width = 1) + coord_polar(theta = "x")
install.packages("ggradar")
library(tidyverse)
df <- data.frame("house" = c("g", "s", "h", "r"),
"values" = c(.5,.1,.1,.3))
ggplot(data = df, mapping = aes(x = house, y = values, color = house, fill = house)) + geom_col(width = 1) + coord_polar(theta = "x")
library(rattle)
rattle()
library(rattle)
rattle()
library(rattle)
rattle()
library(rattle)
rattle()
library(tidyverse)
library(tidytext)
df <- read.csv("Debate3.csv")
df %>% unnest_tokens(words, "text")
View(df)
df %>% unnest_tokens(words, "Text")
df %>% unnest_tokens(word, "Text")
df %>% unnest_tokens(word, "Text")
df %>% str()
df$Text <- as.character(df$Text)
df %>% unnest_tokens(word, "Text")
df %>% unnest_tokens(word, "Text") %>% group_by(Character) %>% count(word, sort = TRUE)
df %>% unnest_tokens(word, "Text") %>%
bind_tf_idf(word, chracter, n)
df %>% unnest_tokens(word, "Text") %>%
count(word) %>%
bind_tf_idf(word, chracter, n)
df %>% unnest_tokens(word, "Text") %>%
count(word) %>%
bind_tf_idf(word, chracter, n)
df %>% unnest_tokens(word, "Text") %>%
count(word) %>%
bind_tf_idf(character, word, n)
df %>% unnest_tokens(word, "Text") %>%
count(word)
df %>% unnest_tokens(word, "Text") %>%
count(word, Character)
df %>% unnest_tokens(word, "Text") %>%
count(word, Character) %>%
bind_tf_idf(word, character, n)
df %>% unnest_tokens(word, "Text") %>%
count(word) %>%
bind_tf_idf(word, character, n)
df %>% unnest_tokens(word, "Text") %>%
count(word) %>%
bind_tf_idf(word, character, n)
df %>% unnest_tokens(word, "Text") %>%
bind_tf_idf(word, character, n)
df %>% unnest_tokens(word, "Text") %>%
count(word, word) %>%
bind_tf_idf(word, character, n)
df %>% unnest_tokens(word, "Text") %>%
count(word, word) %>%
bind_tf_idf(word, character, n)
df %>% unnest_tokens(word, "Text") %>%
count(word, word)
df %>% unnest_tokens(word, "Text") %>%
count(word, word) %>%
bind_tf_idf(word, character, n)
df$Debate <- "Debate"
df %>% unnest_tokens(Text)
df %>% unnest_tokens(word, "Text")
df %>%
unnest_tokens(word, "Text") %>%
count(Debate, word)
df %>%
unnest_tokens(word, "Text") %>%
count(Debate, word) %>%
bind_tf_idf(word, character, n)
df %>%
unnest_tokens(word, "Text") %>%
count(Debate, word) %>%
bind_tf_idf(word, character, n)
df %>%
unnest_tokens(word, "Text") %>%
count(word, character) %>%
bind_tf_idf(word, character, n)
df %>%
unnest_tokens(word, "Text") %>%
count(word, character) %>%
bind_tf_idf(word, character, n)
View(df)
df %>%
unnest_tokens(word, "Text") %>%
count(word, Character) %>%
bind_tf_idf(word, Character, n)
df %>%
unnest_tokens(word, "Text") %>%
count(word, Character) %>%
bind_tf_idf(word, Character, n) %>%
group_by(Character) %>%
top_n(tf_idf, n = 5)
setwd("E:/School/R Work/SortingHat")
library(caret)
library(tidyverse)
df <- readRDS("harrypotter.rds")
df <- df %>% select(-character)
df$TargetHouse <- as.factor(df$TargetHouse)
modelData <- upSample(df %>% select(-TargetHouse), df$TargetHouse)
library(doParallel)
cl <- makePSOCKcluster(7)
registerDoParallel(cl)
L1BaseLineModel <- train(Class~.,
data = modelData,
method = "glmnet",
tuneGrid = expand.grid(alpha = 1,
lambda = 10^seq(-3,3, length = 100))))
L2BaseLineModel <- train(Class~.,
data = modelData,
method = "glmnet",
tuneGrid = expand.grid(alpha = 0,
lambda = 10^seq(-3,3, length = 100))))
ElasticNetModel <- train(Class~.,
data = modelData,
method = "glmnet",
tuneLength = 100)
stopCluster(cl)
cl <- makePSOCKcluster(7)
registerDoParallel(cl)
L1BaseLineModel <- train(Class~.,
data = modelData,
method = "glmnet",
tuneGrid = expand.grid(alpha = 1,
lambda = 10^seq(-3,3, length = 100)))
L2BaseLineModel <- train(Class~.,
data = modelData,
method = "glmnet",
tuneGrid = expand.grid(alpha = 0,
lambda = 10^seq(-3,3, length = 100)))
ElasticNetModel <- train(Class~.,
data = modelData,
method = "glmnet",
tuneLength = 100)
stopCluster(cl)
RegularizationBaseModelSamples <- resamples(list("L1" = L1BaseLineModel, "L2" = L2BaseLineModel, "Elastic Net" = ElasticNetModel))
plot(RegularizationBaseModelSamples)
summary(RegularizationBaseModelSamples)
bwplot(RegularizationBaseModelSamples)
10^seq(-3,3, length = 100)
10^seq(-3,3, length = 1000)
options(scipen = 999)
10^seq(-3,3, length = 1000)
10^seq(-3,3, length = 100)
L1BaseLineModel
ElasticNetModel
plot(RegularizationBaseModelSamples)
summary(RegularizationBaseModelSamples)
bwplot(RegularizationBaseModelSamples)
cl <- makePSOCKcluster(7)
registerDoParallel(cl)
BoxCoxL1BaseLineModel <- train(Class~.,
data = modelData,
method = "glmnet",
preProc = c("BoxCox"),
tuneGrid = expand.grid(alpha = 1,
lambda = 10^seq(-3,3, length = 100)))
YeoJohnsonL1BaseLineModel <- train(Class~.,
data = modelData,
method = "glmnet",
preProc = c("YeoJohnson"),
tuneGrid = expand.grid(alpha = 1,
lambda = 10^seq(-3,3, length = 100)))
CenterScaleL1BaseLineModel <- train(Class~.,
data = modelData,
method = "glmnet",
preProc = c("center","scale"),
tuneGrid = expand.grid(alpha = 1,
lambda = 10^seq(-3,3, length = 100)))
BoxCoxL2BaseLineModel <- train(Class~.,
data = modelData,
method = "glmnet",
preProc = c("BoxCox"),
tuneGrid = expand.grid(alpha = 0,
lambda = 10^seq(-3,3, length = 100)))
YeoJohnsonL2BaseLineModel <- train(Class~.,
data = modelData,
method = "glmnet",
preProc = c("YeoJohnson"),
tuneGrid = expand.grid(alpha = 0,
lambda = 10^seq(-3,3, length = 100)))
CenterScaleL2BaseLineModel <- train(Class~.,
data = modelData,
method = "glmnet",
preProc = c("center","scale"),
tuneGrid = expand.grid(alpha = 0,
lambda = 10^seq(-3,3, length = 100)))
BoxCoxElasticNetModel <- train(Class~.,
data = modelData,
method = "glmnet",
preProc = c("BoxCox"),
tuneLength = 20)
YeoJohnsonElasticNetModel <- train(Class~.,
data = modelData,
method = "glmnet",
preProc = c("YeoJohnson"),
tuneLength = 20)
CenterScaleElasticNetModel <- train(Class~.,
data = modelData,
method = "glmnet",
preProc = c("center","scale"),
tuneLength = 20)
stopCluster(cl)
preProcessL1 <- resamples(list("BoxCox" = BoxCoxL1BaseLineModel, "YeoJohnson" = YeoJohnsonL1BaseLineModel, "CenterScale" = CenterScaleL1BaseLineModel))
prePRocessL2 <- resamples(list("BoxCox" = BoxCoxL2BaseLineModel, "YeoJohnson" = YeoJohnsonL2BaseLineModel, "CenterScale" = CenterScaleL2BaseLineModel))
preProceessElasticNet <- resamples(list("BoxCox" = BoxCoxElasticNetModel, "YeoJohnson" = YeoJohnsonElasticNetModel, "CenterScale" = CenterScaleElasticNetModel))
summary(preProcessL1)
bwplot(preProcessL1
summary(preProcessL2)
bwplot(preProcessL2)
preProcessL2 <- resamples(list("BoxCox" = BoxCoxL2BaseLineModel, "YeoJohnson" = YeoJohnsonL2BaseLineModel, "CenterScale" = CenterScaleL2BaseLineModel))
mary(preProcessL1)
bwplot(preProcessL1)
summary(preProcessL2)
bwplot(preProcessL2)
summary(preProceessElasticNet)
bwplot(preProceessElasticNet)
summary(preProcessL1)
bwplot(preProcessL1)
summary(preProcessL2)
bwplot(preProcessL2)
summary(preProceessElasticNet)
bwplot(preProceessElasticNet)
summary(preProcessL2)
bwplot(preProcessL2)
summary(preProceessElasticNet)
bwplot(preProceessElasticNet)
cl <- makePSOCKcluster(7)
registerDoParallel(cl)
PCACenterScaleL1BaseLineModel <- train(Class~.,
data = modelData,
method = "glmnet",
preProc = c("center","scale", "pca"),
tuneGrid = expand.grid(alpha = 1,
lambda = 10^seq(-3,3, length = 100)),
trControl = trainControl(method = "repeatedcv",
repeats = 10,
number = 10))
